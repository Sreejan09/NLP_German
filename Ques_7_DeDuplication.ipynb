{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlNxx6mK1kDeUoYHi80OCJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sreejan09/NLP_German/blob/main/Ques_7_DeDuplication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOlpNB2rdei7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import hashlib\n",
        "from datasketch import MinHash\n",
        "from simhash import Simhash"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to folder with text files\n",
        "input_folder = \"input_articles\"\n",
        "output_folder = \"filtered_articles\""
      ],
      "metadata": {
        "id": "fGk18sBEeAtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a folder for filtered articles\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)"
      ],
      "metadata": {
        "id": "OcrDpDKVd9HC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to compute SimHash\n",
        "def compute_simhash(text):\n",
        "    return Simhash(text).value\n",
        "\n",
        "# Helper function to compute MinHash\n",
        "def compute_minhash(text, num_perm=128):\n",
        "    m = MinHash(num_perm=num_perm)\n",
        "    for word in text.split():\n",
        "        m.update(word.encode('utf8'))\n",
        "    return m\n",
        "\n",
        "# Function to check similarity between hashes\n",
        "def is_similar(hash1, hash2, threshold=0.8):\n",
        "    return hash1.similarity(hash2) >= threshold"
      ],
      "metadata": {
        "id": "WARkbbkyd73p"
      },
      "execution_count": null,
      "outputs": []
    },
   
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load and deduplicate articles\n",
        "def deduplicate_articles(input_folder, output_folder, method=\"simhash\", similarity_threshold=0.8):\n",
        "    seen_hashes = set()\n",
        "    file_count = 0\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "        file_path = os.path.join(input_folder, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "            if method == \"simhash\":\n",
        "                current_hash = compute_simhash(text)\n",
        "            elif method == \"minhash\":\n",
        "                current_hash = compute_minhash(text)\n",
        "            else:\n",
        "                raise ValueError(\"Method should be either 'simhash' or 'minhash'.\")\n",
        "\n",
        "            # Check similarity against previously seen hashes\n",
        "            is_duplicate = False\n",
        "            for prev_hash in seen_hashes:\n",
        "                if method == \"simhash\":\n",
        "                    # Simhash similarity (bitwise Hamming distance)\n",
        "                    if Simhash(text).distance(Simhash(prev_hash)) < (1 - similarity_threshold) * 64:\n",
        "                        is_duplicate = True\n",
        "                        break\n",
        "                elif method == \"minhash\":\n",
        "                    # Minhash similarity\n",
        "                    if is_similar(current_hash, prev_hash, similarity_threshold):\n",
        "                        is_duplicate = True\n",
        "                        break\n",
        "\n",
        "            if not is_duplicate:\n",
        "                seen_hashes.add(text)  # Store original text for Simhash/Minhash comparison\n",
        "                file_count += 1\n",
        "                # Save the non-duplicate file to the new folder\n",
        "                with open(os.path.join(output_folder, filename), 'w', encoding='utf-8') as output_file:\n",
        "                    output_file.write(text)\n",
        "\n",
        "    print(f\"Deduplication complete! {file_count} unique articles saved to {output_folder}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "hViDacDGd3R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Run the deduplication process\n",
        "deduplicate_articles(input_folder, output_folder, method=\"simhash\", similarity_threshold=0.8)"
      ],
      "metadata": {
        "id": "nYCH8WGPd0UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "453TMKsHdwnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Z4qVOg0dtHf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
